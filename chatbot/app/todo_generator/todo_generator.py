from langchain.chains import ConversationalRetrievalChain
from langchain_openai import OpenAIEmbeddings
from langchain.llms.base import LLM

from pydantic import SecretStr
from typing import Optional, List, Any
import requests
import re
from decouple import config

# === Configuration === #
API_KEY = "S13P12E104-9fba4818-53ba-4f8b-92ce-fcc15166bb33"
BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"
TODO_GENERATOR_LLM_MODEL = "gpt-4.1"
TODO_GENERATOR_EMBED_MODEL = "text-embedding-3-large"


# === LLM Definition === #
class TodoGeneratorLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "TodoGeneratorLLM"

    def _call(
        self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any
    ) -> str:
        url = f"{BASE_URL}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}",
        }
        data = {
            "model": TODO_GENERATOR_LLM_MODEL,
            "messages": [
                {"role": "system", "content": "answer in korean"},
                {"role": "user", "content": prompt},
            ],
            "max_tokens": 20000,
            "temperature": 0.3,
            "response_format": {"type": "json_object"},
        }
        try:
            response = requests.post(url=url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Internal API Error] {str(e)}"


def get_todo_generator_llm() -> TodoGeneratorLLM:
    return TodoGeneratorLLM()


def get_todo_generator_embeddings() -> OpenAIEmbeddings:
    return OpenAIEmbeddings(
        model=TODO_GENERATOR_EMBED_MODEL,
        api_key=SecretStr(str(API_KEY)),
        base_url=str(BASE_URL),
    )


# === Text cleaning === #
def clean_text(text: str) -> str:
    text = text.replace("\n", " ").strip()
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9 .,!?]", " ", text)
    return text


# === Generating Todos === #
def generate_todos(material_text: str, lecture_info: Optional[str] = None) -> str:
    """Generates a structured to-do list in JSON format based on the provided material and lecture info."""

    prompt_template = """\
You are an assistant that generates STRICT JSON to-do plans for cooking lectures.
- Output MUST be valid JSON that strictly follows the schema.
- Do not add any extra keys.
- Do not include markdown, code fences, or explanations.
- Korean language for all text values.
Schema:
[
    {
        "sequence": 1,
        "title": "string",
        "summary": "string",
        "chapters": [
            {
                "chapterName": "string",
                "sequence": 1,
                "numOfTodos": 2,
                "todos": [
                    {
                        "title": "ì¬ë£Œ ì†ì§ˆí•˜ê¸°",
                        "type": "NORMAL",
                        "seconds": null,
                        "sequence": 1
                    },
                    {
                        "title": "5ë¶„ê°„ ë“ì´ê¸°",
                        "type": "TIMER",
                        "seconds": 300,
                        "sequence": 2
                    }
                ]
            }
        ]
    }
]
ë‹¤ìŒì€ ê°•ì˜ êµì•ˆ(ìë£Œ)ì™€ ê°•ì˜ ë©”íƒ€ì •ë³´ë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²´ê³„ì ì¸ TODO JSONì„ ìƒì„±í•˜ë¼.
- sectionì€ (ì¬ë£Œ ì¤€ë¹„/ì‚¬ì „ ì¤€ë¹„/ì¡°ë¦¬/í”Œë ˆì´íŒ…/ì •ë¦¬/í‰ê°€ ë“±) ë…¼ë¦¬ë³„ë¡œ ì ì ˆíˆ ë‚˜ëˆŒ ê²ƒ
- ê° todoëŠ” ê°€ëŠ¥í•œ 5~20ë¶„ ë‹¨ìœ„ë¡œ ìª¼ê°¤ ê²ƒ
- ê° í•­ëª©ì— 1ë¶€í„° ìˆœì„œëŒ€ë¡œ sequenceë¥¼ ë°°ì¹˜í•  ê²ƒ
- ê° í•­ëª©ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì¶”ì •í•˜ì—¬ secondsë¡œ í™˜ì‚°í•˜ì—¬ ì‘ì„±í•  ê²ƒ
- secondsê°€ ìˆì„ ê²½ìš° typeì€ TIMERì—¬ì•¼ í•˜ê³ , typeì´ NORMALì¼ ê²½ìš°ëŠ” secondsëŠ” nullë¡œ ì‘ì„±í•  ê²ƒ
- JSONë§Œ ì¶œë ¥

[ê°•ì˜ ì •ë³´]
{lecture_info}

[ê°•ì˜ ìë£Œ]
{material_text}
"""
    prompt = prompt_template.replace(
        "{lecture_info}", lecture_info or "ì œê³µë˜ì§€ ì•ŠìŒ"
    ).replace("{material_text}", material_text)

    llm = get_todo_generator_llm()
    result = llm._call(prompt)

    print("ğŸ¤– LLM Response:", result)
    return result
