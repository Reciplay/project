from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.llms.base import LLM

from pydantic import SecretStr
from typing import Optional, List, Any
import requests
import re
from decouple import config
import os
import pdfplumber

# === Configuration === #
API_KEY = "S13P12E104-9fba4818-53ba-4f8b-92ce-fcc15166bb33"
BASE_URL = "https://gms.ssafy.io/gmsapi/api.openai.com/v1"
CHATBOT_LLM_MODEL = "gpt-4.1-mini"
EMBED_MODEL = "text-embedding-3-small"


# === LLM Definition === #
class ChatbotLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "MycompanyLLM"

    def _call(
        self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any
    ) -> str:
        url = f"{BASE_URL}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}",
        }
        data = {
            "model": CHATBOT_LLM_MODEL,
            "messages": [
                {"role": "system", "content": "answer in korean"},
                {"role": "user", "content": prompt},
            ],
            "max_tokens": 4096,
            "temperature": 0.3,
        }
        try:
            response = requests.post(url=url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            return f"[Internal API Error] {str(e)}"


def get_chatbot_llm() -> ChatbotLLM:
    return ChatbotLLM()


def get_embeddings() -> OpenAIEmbeddings:
    return OpenAIEmbeddings(
        model=EMBED_MODEL,
        api_key=SecretStr(str(API_KEY)),
        base_url=str(BASE_URL),
    )


# === Text cleaning === #
def clean_text(text: str) -> str:
    text = text.replace("\n", " ").strip()
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9 .,!?]", " ", text)
    return text


# === Save vector index === #
def save_doc(path: str, index_path: str):
    docs = []
    if path.endswith(".pdf"):
        try:
            with pdfplumber.open(path) as pdf:
                content = "\n".join(page.extract_text() or "" for page in pdf.pages)
                docs.append(Document(page_content=content, metadata={"source": path}))
        except Exception as e:
            print(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return
    else:
        loader = TextLoader(path, encoding="utf-8")
        docs = loader.load()

    for i, doc in enumerate(docs, 1):
        print(f"--- ë¬¸ì„œ {i} ---\n{doc.page_content[:500]}\n")

    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)

    for chunk in chunks:
        chunk.page_content = clean_text(chunk.page_content)
        chunk.metadata["source"] = path

    embedding = get_embeddings()

    if os.path.exists(index_path):
        db = FAISS.load_local(
            index_path, embedding, allow_dangerous_deserialization=True
        )
        db.add_documents(chunks)
    else:
        db = FAISS.from_documents(chunks, embedding)
    db.save_local(index_path)
    print(f"ì €ì¥ ì„±ê³µ, {index_path}")


# === Run chatbot === #
def run_chatbot(
    memory: ConversationBufferMemory,
    user_input: str,
    index_path: str,
):
    embedding = get_embeddings()
    db = FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)
    retriever = db.as_retriever()

    prompt = PromptTemplate(
        input_variables=["context", "chat_history", "question"],
        template="""
ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ AI ìš”ë¦¬ ë³´ì¡° ê°•ì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ì°¸ê³  ë¬¸ì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë§Œì•½ ì°¸ê³  ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì§ˆë¬¸í•  ë•Œ, ìš”ë¦¬ì™€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ë©´ ê·¸ëŒ€ë¡œ ë‹µë³€í•˜ê³ , ìš”ë¦¬ê³¼ ê´€ë ¨ ìˆëŠ” ë‚´ìš©ì€ ë¬¸ì„œì— ì—†ì–´ì„œ ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ëŒ€í™” íˆìŠ¤í† ë¦¬]
{chat_history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

[AIì˜ ë‹µë³€]
(ì°¸ê³  ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë¬¸ë‹¨ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.)
""",
    )

    qa = ConversationalRetrievalChain.from_llm(
        llm=get_chatbot_llm(),
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt},
        return_source_documents=True,
        output_key="answer",
        verbose=False,
    )

    result = qa.invoke({"question": user_input})
    print("ğŸ¤– ì±—ë´‡:", result["answer"])
    return result["answer"]
